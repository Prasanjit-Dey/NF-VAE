{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ge8zaa46Gug",
        "outputId": "3a33cbf2-3374-4b9e-a214-64cd162df10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "8wGoR_Qh9xs1",
        "outputId": "e3e68985-3daf-4572-aec9-68ffc0afe7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-194b7633-9d69-4927-b52a-88bf10d87c69\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-194b7633-9d69-4927-b52a-88bf10d87c69\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined_matrix.csv to combined_matrix.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "npeEbAh39dCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_combined_matrix = pd.read_csv(\"combined_matrix.csv\")"
      ],
      "metadata": {
        "id": "BswMznDyBg2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_combined_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "crrHaHC7BsD_",
        "outputId": "7bff213f-6d33-458f-a039-1dc1d92792f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   parameter_location_id  2019-10-28T02:00:00+08:00  \\\n",
              "0                co_6168                     1400.0   \n",
              "1               no2_6168                       73.0   \n",
              "2                o3_6168                       49.0   \n",
              "3              pm10_6168                      141.0   \n",
              "4              pm25_6168                       80.0   \n",
              "5               so2_6168                       10.0   \n",
              "6                co_6169                     1400.0   \n",
              "7               no2_6169                       61.0   \n",
              "8                o3_6169                       52.0   \n",
              "9              pm10_6169                      105.0   \n",
              "10             pm25_6169                       59.0   \n",
              "11              so2_6169                        7.0   \n",
              "12               co_6167                     1300.0   \n",
              "13              no2_6167                       69.0   \n",
              "14               o3_6167                       76.0   \n",
              "15             pm10_6167                      113.0   \n",
              "16             pm25_6167                       72.0   \n",
              "17              so2_6167                        7.0   \n",
              "18               co_6218                     1300.0   \n",
              "19              no2_6218                       42.0   \n",
              "20               o3_6218                       62.0   \n",
              "21             pm10_6218                      118.0   \n",
              "22             pm25_6218                       66.0   \n",
              "23              so2_6218                        4.0   \n",
              "24               co_6274                      700.0   \n",
              "25              no2_6274                       44.0   \n",
              "26               o3_6274                      109.0   \n",
              "27             pm10_6274                      112.0   \n",
              "28             pm25_6274                       60.0   \n",
              "29              so2_6274                       15.0   \n",
              "30               co_6273                      600.0   \n",
              "31              no2_6273                       60.0   \n",
              "32               o3_6273                      109.0   \n",
              "33             pm10_6273                      122.0   \n",
              "34             pm25_6273                       75.0   \n",
              "35              so2_6273                       13.0   \n",
              "\n",
              "    2018-05-28T12:00:00+08:00  2018-02-23T17:00:00+08:00  \\\n",
              "0                       769.0                      680.0   \n",
              "1                        65.0                       44.0   \n",
              "2                        78.0                       76.0   \n",
              "3                       187.0                       45.0   \n",
              "4                        57.0                       29.0   \n",
              "5                        14.0                        5.0   \n",
              "6                       809.0                      905.0   \n",
              "7                        50.0                       23.0   \n",
              "8                        91.0                       99.0   \n",
              "9                       202.0                        0.0   \n",
              "10                       61.0                       32.0   \n",
              "11                       13.0                        7.0   \n",
              "12                      951.0                      596.0   \n",
              "13                       50.0                       13.0   \n",
              "14                       90.0                      118.0   \n",
              "15                      175.0                       43.0   \n",
              "16                       58.0                       30.0   \n",
              "17                       10.0                        5.0   \n",
              "18                      674.0                      539.0   \n",
              "19                       32.0                       37.0   \n",
              "20                      115.0                       84.0   \n",
              "21                      204.0                       78.0   \n",
              "22                       81.0                       43.0   \n",
              "23                       15.0                        7.0   \n",
              "24                      474.0                      746.0   \n",
              "25                       17.0                       24.0   \n",
              "26                      176.0                      111.0   \n",
              "27                      182.0                       68.0   \n",
              "28                       41.0                       36.0   \n",
              "29                       18.0                       18.0   \n",
              "30                      730.0                      624.0   \n",
              "31                       14.0                       24.0   \n",
              "32                      190.0                      109.0   \n",
              "33                      181.0                       74.0   \n",
              "34                       73.0                       43.0   \n",
              "35                       19.0                       13.0   \n",
              "\n",
              "    2018-03-25T14:00:00+08:00  2019-11-14T11:00:00+08:00  \\\n",
              "0                       796.0                      400.0   \n",
              "1                        49.0                       55.0   \n",
              "2                       113.0                       63.0   \n",
              "3                         0.0                       99.0   \n",
              "4                       103.0                       32.0   \n",
              "5                        15.0                        7.0   \n",
              "6                       630.0                      500.0   \n",
              "7                        23.0                       41.0   \n",
              "8                       145.0                       68.0   \n",
              "9                        65.0                       60.0   \n",
              "10                       62.0                       22.0   \n",
              "11                       11.0                        8.0   \n",
              "12                      933.0                      300.0   \n",
              "13                       32.0                       28.0   \n",
              "14                      131.0                       69.0   \n",
              "15                       93.0                        0.0   \n",
              "16                       82.0                       30.0   \n",
              "17                       10.0                        9.0   \n",
              "18                      650.0                      900.0   \n",
              "19                       42.0                       38.0   \n",
              "20                      148.0                       96.0   \n",
              "21                      137.0                      132.0   \n",
              "22                       80.0                       43.0   \n",
              "23                       13.0                       16.0   \n",
              "24                     1016.0                      600.0   \n",
              "25                       23.0                       43.0   \n",
              "26                      261.0                       85.0   \n",
              "27                      176.0                      102.0   \n",
              "28                      117.0                       49.0   \n",
              "29                       16.0                        7.0   \n",
              "30                     1096.0                      400.0   \n",
              "31                       30.0                       30.0   \n",
              "32                      243.0                       82.0   \n",
              "33                      156.0                      117.0   \n",
              "34                      124.0                       64.0   \n",
              "35                       26.0                       14.0   \n",
              "\n",
              "    2021-06-01T17:00:00+08:00  2018-09-09T12:00:00+08:00  \\\n",
              "0                       700.0                      363.0   \n",
              "1                        22.0                       14.0   \n",
              "2                       243.0                      130.0   \n",
              "3                        63.0                       33.0   \n",
              "4                        24.0                        8.0   \n",
              "5                         8.0                        3.0   \n",
              "6                       600.0                      566.0   \n",
              "7                        24.0                       20.0   \n",
              "8                       243.0                      143.0   \n",
              "9                        83.0                       42.0   \n",
              "10                       31.0                        9.0   \n",
              "11                        7.0                        3.0   \n",
              "12                      700.0                      575.0   \n",
              "13                        0.0                        8.0   \n",
              "14                      233.0                      148.0   \n",
              "15                       78.0                       36.0   \n",
              "16                       31.0                       13.0   \n",
              "17                        5.0                        5.0   \n",
              "18                      500.0                      279.0   \n",
              "19                       18.0                       10.0   \n",
              "20                      193.0                      127.0   \n",
              "21                       79.0                       22.0   \n",
              "22                       16.0                        6.0   \n",
              "23                        8.0                        7.0   \n",
              "24                      900.0                      575.0   \n",
              "25                        7.0                       14.0   \n",
              "26                      267.0                      157.0   \n",
              "27                       92.0                       63.0   \n",
              "28                       36.0                       28.0   \n",
              "29                        4.0                       14.0   \n",
              "30                      800.0                      706.0   \n",
              "31                       13.0                       16.0   \n",
              "32                      277.0                      173.0   \n",
              "33                      127.0                       76.0   \n",
              "34                       34.0                       35.0   \n",
              "35                        5.0                       19.0   \n",
              "\n",
              "    2018-12-04T01:00:00+08:00  2019-05-22T20:00:00+08:00  ...  \\\n",
              "0                      2000.0                      800.0  ...   \n",
              "1                        45.0                       97.0  ...   \n",
              "2                        21.0                      199.0  ...   \n",
              "3                         0.0                      113.0  ...   \n",
              "4                        25.0                       29.0  ...   \n",
              "5                         3.0                        4.0  ...   \n",
              "6                      1900.0                     1000.0  ...   \n",
              "7                        36.0                       51.0  ...   \n",
              "8                        15.0                      170.0  ...   \n",
              "9                         0.0                      141.0  ...   \n",
              "10                       27.0                       33.0  ...   \n",
              "11                        1.0                        8.0  ...   \n",
              "12                     2300.0                      800.0  ...   \n",
              "13                       54.0                       70.0  ...   \n",
              "14                       25.0                      201.0  ...   \n",
              "15                        0.0                      167.0  ...   \n",
              "16                       30.0                       28.0  ...   \n",
              "17                        2.0                        5.0  ...   \n",
              "18                     1900.0                      700.0  ...   \n",
              "19                       63.0                       19.0  ...   \n",
              "20                       15.0                      189.0  ...   \n",
              "21                        0.0                       64.0  ...   \n",
              "22                       19.0                       32.0  ...   \n",
              "23                       10.0                       13.0  ...   \n",
              "24                     1200.0                      800.0  ...   \n",
              "25                       45.0                       50.0  ...   \n",
              "26                       29.0                      182.0  ...   \n",
              "27                        0.0                      122.0  ...   \n",
              "28                       50.0                       48.0  ...   \n",
              "29                       16.0                       11.0  ...   \n",
              "30                     1800.0                      400.0  ...   \n",
              "31                       50.0                       35.0  ...   \n",
              "32                       38.0                      190.0  ...   \n",
              "33                       43.0                       98.0  ...   \n",
              "34                       31.0                       39.0  ...   \n",
              "35                        7.0                       13.0  ...   \n",
              "\n",
              "    2019-08-13T15:00:00+08:00  2020-12-07T18:00:00+08:00  \\\n",
              "0                       400.0                      800.0   \n",
              "1                        22.0                       63.0   \n",
              "2                       182.0                       49.0   \n",
              "3                        27.0                       72.0   \n",
              "4                        10.0                       70.0   \n",
              "5                         4.0                        2.0   \n",
              "6                       600.0                      800.0   \n",
              "7                        11.0                       45.0   \n",
              "8                       146.0                       58.0   \n",
              "9                        39.0                       67.0   \n",
              "10                       21.0                       62.0   \n",
              "11                        2.0                        7.0   \n",
              "12                      500.0                     1000.0   \n",
              "13                        8.0                       83.0   \n",
              "14                      154.0                       43.0   \n",
              "15                       16.0                       91.0   \n",
              "16                        9.0                       70.0   \n",
              "17                        5.0                       11.0   \n",
              "18                      600.0                     1000.0   \n",
              "19                        5.0                       64.0   \n",
              "20                      120.0                       61.0   \n",
              "21                        8.0                       96.0   \n",
              "22                        8.0                       84.0   \n",
              "23                        4.0                       22.0   \n",
              "24                      700.0                     1300.0   \n",
              "25                        5.0                       51.0   \n",
              "26                       97.0                       78.0   \n",
              "27                        9.0                      105.0   \n",
              "28                       10.0                      142.0   \n",
              "29                        3.0                       24.0   \n",
              "30                      500.0                        0.0   \n",
              "31                        3.0                        0.0   \n",
              "32                       96.0                       73.0   \n",
              "33                        4.0                        0.0   \n",
              "34                        2.0                        0.0   \n",
              "35                        4.0                        0.0   \n",
              "\n",
              "    2021-01-15T20:00:00+08:00  2021-03-24T13:00:00+08:00  \\\n",
              "0                      1100.0                      500.0   \n",
              "1                        89.0                       36.0   \n",
              "2                        49.0                       91.0   \n",
              "3                       230.0                       63.0   \n",
              "4                        87.0                       14.0   \n",
              "5                        16.0                        8.0   \n",
              "6                      1000.0                      600.0   \n",
              "7                        77.0                       27.0   \n",
              "8                        56.0                      114.0   \n",
              "9                       203.0                       61.0   \n",
              "10                       90.0                       22.0   \n",
              "11                       11.0                        7.0   \n",
              "12                     1000.0                      600.0   \n",
              "13                       70.0                       30.0   \n",
              "14                       65.0                      110.0   \n",
              "15                      304.0                        0.0   \n",
              "16                       88.0                       24.0   \n",
              "17                       14.0                        0.0   \n",
              "18                     1200.0                      700.0   \n",
              "19                       75.0                       12.0   \n",
              "20                       82.0                      113.0   \n",
              "21                      338.0                       43.0   \n",
              "22                      123.0                       23.0   \n",
              "23                       22.0                        3.0   \n",
              "24                     1000.0                      400.0   \n",
              "25                       56.0                        8.0   \n",
              "26                       64.0                      106.0   \n",
              "27                      331.0                      109.0   \n",
              "28                      103.0                       33.0   \n",
              "29                       16.0                        6.0   \n",
              "30                     1100.0                      400.0   \n",
              "31                       55.0                       10.0   \n",
              "32                       68.0                      119.0   \n",
              "33                      345.0                       94.0   \n",
              "34                       87.0                       28.0   \n",
              "35                       12.0                        6.0   \n",
              "\n",
              "    2018-08-17T02:00:00+08:00  2020-02-08T22:00:00+08:00  \\\n",
              "0                       752.0                      300.0   \n",
              "1                        12.0                       17.0   \n",
              "2                        51.0                       60.0   \n",
              "3                        35.0                       26.0   \n",
              "4                        27.0                       22.0   \n",
              "5                         7.0                        4.0   \n",
              "6                       596.0                      500.0   \n",
              "7                        11.0                       22.0   \n",
              "8                        56.0                       62.0   \n",
              "9                        38.0                       28.0   \n",
              "10                       20.0                       28.0   \n",
              "11                        3.0                        3.0   \n",
              "12                      568.0                      600.0   \n",
              "13                       15.0                       25.0   \n",
              "14                       55.0                       65.0   \n",
              "15                       31.0                       33.0   \n",
              "16                       25.0                       27.0   \n",
              "17                        3.0                        5.0   \n",
              "18                      308.0                      900.0   \n",
              "19                        8.0                       23.0   \n",
              "20                       53.0                       74.0   \n",
              "21                        9.0                       52.0   \n",
              "22                        4.0                       44.0   \n",
              "23                       15.0                        8.0   \n",
              "24                      611.0                      700.0   \n",
              "25                       24.0                       33.0   \n",
              "26                       67.0                       73.0   \n",
              "27                       40.0                       87.0   \n",
              "28                       25.0                       68.0   \n",
              "29                       20.0                        7.0   \n",
              "30                      790.0                      900.0   \n",
              "31                       17.0                       20.0   \n",
              "32                       61.0                       70.0   \n",
              "33                       56.0                       78.0   \n",
              "34                       40.0                       71.0   \n",
              "35                       10.0                        5.0   \n",
              "\n",
              "    2019-04-18T07:00:00+08:00  2020-12-28T06:00:00+08:00  \\\n",
              "0                       800.0                     1100.0   \n",
              "1                        80.0                       68.0   \n",
              "2                       137.0                       37.0   \n",
              "3                        84.0                       58.0   \n",
              "4                        13.0                       82.0   \n",
              "5                         5.0                        1.0   \n",
              "6                       800.0                      800.0   \n",
              "7                        40.0                       62.0   \n",
              "8                       138.0                       47.0   \n",
              "9                        59.0                       72.0   \n",
              "10                       29.0                      100.0   \n",
              "11                        6.0                        7.0   \n",
              "12                      800.0                     1800.0   \n",
              "13                       44.0                       67.0   \n",
              "14                      146.0                       55.0   \n",
              "15                       91.0                       76.0   \n",
              "16                       27.0                       84.0   \n",
              "17                        4.0                        5.0   \n",
              "18                     1400.0                     1300.0   \n",
              "19                       71.0                       53.0   \n",
              "20                      153.0                      128.0   \n",
              "21                       94.0                       40.0   \n",
              "22                       48.0                       26.0   \n",
              "23                       42.0                        4.0   \n",
              "24                      500.0                     1000.0   \n",
              "25                       27.0                       32.0   \n",
              "26                      148.0                      104.0   \n",
              "27                       79.0                      216.0   \n",
              "28                       45.0                      154.0   \n",
              "29                       11.0                       12.0   \n",
              "30                      900.0                        0.0   \n",
              "31                       31.0                        0.0   \n",
              "32                      152.0                       56.0   \n",
              "33                       72.0                        0.0   \n",
              "34                       50.0                        0.0   \n",
              "35                       15.0                        0.0   \n",
              "\n",
              "    2020-04-14T11:00:00+08:00  2021-01-09T12:00:00+08:00  \n",
              "0                       500.0                      500.0  \n",
              "1                        46.0                       29.0  \n",
              "2                       131.0                       53.0  \n",
              "3                        55.0                       36.0  \n",
              "4                        46.0                       18.0  \n",
              "5                         9.0                        8.0  \n",
              "6                       400.0                      400.0  \n",
              "7                        29.0                       19.0  \n",
              "8                       142.0                       56.0  \n",
              "9                        68.0                       32.0  \n",
              "10                       37.0                       17.0  \n",
              "11                       12.0                        8.0  \n",
              "12                      600.0                      500.0  \n",
              "13                       24.0                       23.0  \n",
              "14                      148.0                       62.0  \n",
              "15                       62.0                       47.0  \n",
              "16                       30.0                       22.0  \n",
              "17                       13.0                       10.0  \n",
              "18                     1200.0                      600.0  \n",
              "19                       39.0                       18.0  \n",
              "20                      137.0                       70.0  \n",
              "21                      127.0                       71.0  \n",
              "22                       58.0                       21.0  \n",
              "23                       14.0                        6.0  \n",
              "24                        0.0                      700.0  \n",
              "25                       29.0                       31.0  \n",
              "26                      156.0                       61.0  \n",
              "27                       85.0                      108.0  \n",
              "28                       44.0                       43.0  \n",
              "29                        5.0                       15.0  \n",
              "30                      400.0                      800.0  \n",
              "31                       18.0                       37.0  \n",
              "32                      151.0                       61.0  \n",
              "33                       87.0                       98.0  \n",
              "34                       56.0                       52.0  \n",
              "35                        9.0                       13.0  \n",
              "\n",
              "[36 rows x 25942 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66af6fa8-7aa1-45d4-9990-c56f61f1e7c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parameter_location_id</th>\n",
              "      <th>2019-10-28T02:00:00+08:00</th>\n",
              "      <th>2018-05-28T12:00:00+08:00</th>\n",
              "      <th>2018-02-23T17:00:00+08:00</th>\n",
              "      <th>2018-03-25T14:00:00+08:00</th>\n",
              "      <th>2019-11-14T11:00:00+08:00</th>\n",
              "      <th>2021-06-01T17:00:00+08:00</th>\n",
              "      <th>2018-09-09T12:00:00+08:00</th>\n",
              "      <th>2018-12-04T01:00:00+08:00</th>\n",
              "      <th>2019-05-22T20:00:00+08:00</th>\n",
              "      <th>...</th>\n",
              "      <th>2019-08-13T15:00:00+08:00</th>\n",
              "      <th>2020-12-07T18:00:00+08:00</th>\n",
              "      <th>2021-01-15T20:00:00+08:00</th>\n",
              "      <th>2021-03-24T13:00:00+08:00</th>\n",
              "      <th>2018-08-17T02:00:00+08:00</th>\n",
              "      <th>2020-02-08T22:00:00+08:00</th>\n",
              "      <th>2019-04-18T07:00:00+08:00</th>\n",
              "      <th>2020-12-28T06:00:00+08:00</th>\n",
              "      <th>2020-04-14T11:00:00+08:00</th>\n",
              "      <th>2021-01-09T12:00:00+08:00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>co_6168</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>769.0</td>\n",
              "      <td>680.0</td>\n",
              "      <td>796.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>363.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>...</td>\n",
              "      <td>400.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>752.0</td>\n",
              "      <td>300.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>no2_6168</td>\n",
              "      <td>73.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>29.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>o3_6168</td>\n",
              "      <td>49.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>...</td>\n",
              "      <td>182.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>53.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pm10_6168</td>\n",
              "      <td>141.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>...</td>\n",
              "      <td>27.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pm25_6168</td>\n",
              "      <td>80.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>so2_6168</td>\n",
              "      <td>10.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>co_6169</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>905.0</td>\n",
              "      <td>630.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>566.0</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>600.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>596.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>no2_6169</td>\n",
              "      <td>61.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>...</td>\n",
              "      <td>11.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>o3_6169</td>\n",
              "      <td>52.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>...</td>\n",
              "      <td>146.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>56.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>pm10_6169</td>\n",
              "      <td>105.0</td>\n",
              "      <td>202.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>...</td>\n",
              "      <td>39.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>pm25_6169</td>\n",
              "      <td>59.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>...</td>\n",
              "      <td>21.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>so2_6169</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>co_6167</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>951.0</td>\n",
              "      <td>596.0</td>\n",
              "      <td>933.0</td>\n",
              "      <td>300.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>2300.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>...</td>\n",
              "      <td>500.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>568.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>no2_6167</td>\n",
              "      <td>69.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>o3_6167</td>\n",
              "      <td>76.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>...</td>\n",
              "      <td>154.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>146.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>pm10_6167</td>\n",
              "      <td>113.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>47.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>pm25_6167</td>\n",
              "      <td>72.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>so2_6167</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>co_6218</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>674.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>279.0</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>...</td>\n",
              "      <td>600.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>308.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>no2_6218</td>\n",
              "      <td>42.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>o3_6218</td>\n",
              "      <td>62.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>...</td>\n",
              "      <td>120.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>pm10_6218</td>\n",
              "      <td>118.0</td>\n",
              "      <td>204.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>338.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>71.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>pm25_6218</td>\n",
              "      <td>66.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>so2_6218</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>co_6274</td>\n",
              "      <td>700.0</td>\n",
              "      <td>474.0</td>\n",
              "      <td>746.0</td>\n",
              "      <td>1016.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>...</td>\n",
              "      <td>700.0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>611.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>no2_6274</td>\n",
              "      <td>44.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>o3_6274</td>\n",
              "      <td>109.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>261.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>267.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>61.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>pm10_6274</td>\n",
              "      <td>112.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>108.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>pm25_6274</td>\n",
              "      <td>60.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>so2_6274</td>\n",
              "      <td>15.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>co_6273</td>\n",
              "      <td>600.0</td>\n",
              "      <td>730.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>1096.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>706.0</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>...</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>400.0</td>\n",
              "      <td>800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>no2_6273</td>\n",
              "      <td>60.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>o3_6273</td>\n",
              "      <td>109.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>61.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>pm10_6273</td>\n",
              "      <td>122.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>345.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>pm25_6273</td>\n",
              "      <td>75.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>so2_6273</td>\n",
              "      <td>13.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36 rows  25942 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66af6fa8-7aa1-45d4-9990-c56f61f1e7c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66af6fa8-7aa1-45d4-9990-c56f61f1e7c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66af6fa8-7aa1-45d4-9990-c56f61f1e7c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a78ce5d9-9ead-4cb5-a646-33608ffcb836\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a78ce5d9-9ead-4cb5-a646-33608ffcb836')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a78ce5d9-9ead-4cb5-a646-33608ffcb836 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_71232f49-b0ce-47a7-bbe4-a14c1917ada8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sorted_combined_matrix')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_71232f49-b0ce-47a7-bbe4-a14c1917ada8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('sorted_combined_matrix');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_combined_matrix = sorted_combined_matrix.set_index('parameter_location_id')\n",
        "\n",
        "sorted_combined_matrix_T =sorted_combined_matrix.T\n",
        "sorted_combined_matrix_T.replace(to_replace=0, method='ffill', inplace=True)\n",
        "sorted_combined_matrix_T.index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_hRP-xUBwn8",
        "outputId": "3be05dc4-6a0b-4e2a-dfca-5e8084b6f927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['2019-10-28T02:00:00+08:00', '2018-05-28T12:00:00+08:00',\n",
              "       '2018-02-23T17:00:00+08:00', '2018-03-25T14:00:00+08:00',\n",
              "       '2019-11-14T11:00:00+08:00', '2021-06-01T17:00:00+08:00',\n",
              "       '2018-09-09T12:00:00+08:00', '2018-12-04T01:00:00+08:00',\n",
              "       '2019-05-22T20:00:00+08:00', '2020-05-07T23:00:00+08:00',\n",
              "       ...\n",
              "       '2019-08-13T15:00:00+08:00', '2020-12-07T18:00:00+08:00',\n",
              "       '2021-01-15T20:00:00+08:00', '2021-03-24T13:00:00+08:00',\n",
              "       '2018-08-17T02:00:00+08:00', '2020-02-08T22:00:00+08:00',\n",
              "       '2019-04-18T07:00:00+08:00', '2020-12-28T06:00:00+08:00',\n",
              "       '2020-04-14T11:00:00+08:00', '2021-01-09T12:00:00+08:00'],\n",
              "      dtype='object', length=25941)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the combined_matrix data\n",
        "normalized_data = sorted_combined_matrix_T\n",
        "scaler = StandardScaler()\n",
        "scaled_dataset = scaler.fit_transform(normalized_data)"
      ],
      "metadata": {
        "id": "ULQiKpnSB3hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = ['adam', 'rmsprop']\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh', 'elu', 'selu', 'softmax', 'swish']\n",
        "mse_values = []\n",
        "window_size = 3"
      ],
      "metadata": {
        "id": "jek9llKqmq3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create supervised data\n",
        "def to_supervised(train):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(window_size, len(train)):\n",
        "        X.append(train[i - window_size:i, :])\n",
        "        Y.append(train[i, 0:1])\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "D34Gg6ub-i-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for optimizer in optimizers:\n",
        "    for activation in activation_functions:\n",
        "        # Load or create your 'scaled_dataset' here\n",
        "\n",
        "        # Create supervised data\n",
        "        X, Y = to_supervised(scaled_dataset)\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        test_size = 0.2\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
        "\n",
        "        # Build the LSTM model\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units=100, return_sequences=True, input_shape=(window_size, X_train.shape[2]), activation=activation))\n",
        "        model.add(LSTM(units=100, return_sequences=True, activation=activation))\n",
        "        model.add(LSTM(units=100, activation=activation))\n",
        "        model.add(Dense(units=100))\n",
        "        model.add(Dense(units=X_train.shape[2]))\n",
        "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "        # Train the model\n",
        "        lstm_history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=256, verbose=2)\n",
        "\n",
        "        # Make predictions and calculate MSE\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_reduced = y_pred.mean(axis=1)\n",
        "        mse = mean_squared_error(Y_test, y_pred_reduced)\n",
        "\n",
        "        print(\"Average MSE for the \" + str(optimizer) + \" optimizer and \" + str(activation) + \" activation function:\", mse)\n",
        "        mse_values.append(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duW4xMra-lvp",
        "outputId": "b30742ab-81ab-413d-ba2d-bf6af5ba8565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9761 - val_loss: 1.0962 - 6s/epoch - 73ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9759 - val_loss: 1.1043 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9752 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9744 - val_loss: 1.1015 - 1s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9724 - val_loss: 1.0996 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9696 - val_loss: 1.0984 - 1s/epoch - 18ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 2s - loss: 0.9667 - val_loss: 1.1000 - 2s/epoch - 18ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9619 - val_loss: 1.1180 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9564 - val_loss: 1.1197 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9451 - val_loss: 1.1264 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9368 - val_loss: 1.1263 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9172 - val_loss: 1.1218 - 1s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.8918 - val_loss: 1.1945 - 1s/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.8624 - val_loss: 1.1697 - 1s/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.8472 - val_loss: 1.1713 - 1s/epoch - 17ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.8214 - val_loss: 1.1901 - 1s/epoch - 18ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.7735 - val_loss: 1.1591 - 1s/epoch - 16ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.7373 - val_loss: 1.2188 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.7045 - val_loss: 1.2066 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.6703 - val_loss: 1.2203 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.6418 - val_loss: 1.3198 - 1s/epoch - 17ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.6334 - val_loss: 1.3295 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.5973 - val_loss: 1.3374 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.5598 - val_loss: 1.2674 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.5326 - val_loss: 1.2778 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.5162 - val_loss: 1.4685 - 1s/epoch - 16ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.4956 - val_loss: 1.5673 - 1s/epoch - 16ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.4845 - val_loss: 1.3601 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.4504 - val_loss: 1.3929 - 1s/epoch - 16ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.4216 - val_loss: 1.3766 - 1s/epoch - 16ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.4174 - val_loss: 1.4834 - 1s/epoch - 16ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.3953 - val_loss: 1.3823 - 1s/epoch - 16ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.3774 - val_loss: 1.4715 - 1s/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.3532 - val_loss: 1.4652 - 1s/epoch - 17ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.3369 - val_loss: 1.5073 - 1s/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.3216 - val_loss: 1.4935 - 1s/epoch - 16ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.3135 - val_loss: 1.5028 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.2903 - val_loss: 1.4954 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.2726 - val_loss: 1.5064 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.2583 - val_loss: 1.5974 - 1s/epoch - 18ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.2455 - val_loss: 1.5959 - 1s/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.2308 - val_loss: 1.5534 - 1s/epoch - 18ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.2115 - val_loss: 1.5402 - 1s/epoch - 18ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.1983 - val_loss: 1.7251 - 1s/epoch - 16ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.1886 - val_loss: 1.5968 - 1s/epoch - 16ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.1759 - val_loss: 1.6003 - 1s/epoch - 16ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.1608 - val_loss: 1.5856 - 1s/epoch - 16ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.1555 - val_loss: 1.6012 - 1s/epoch - 16ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.1413 - val_loss: 1.5494 - 1s/epoch - 16ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.1357 - val_loss: 1.6710 - 1s/epoch - 16ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.1272 - val_loss: 1.6473 - 1s/epoch - 16ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.1215 - val_loss: 1.6181 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.1122 - val_loss: 1.6164 - 1s/epoch - 16ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.1149 - val_loss: 1.6825 - 1s/epoch - 16ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.1004 - val_loss: 1.6485 - 1s/epoch - 16ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.0906 - val_loss: 1.6524 - 1s/epoch - 16ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.0823 - val_loss: 1.6593 - 1s/epoch - 16ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.0763 - val_loss: 1.6860 - 1s/epoch - 16ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.0766 - val_loss: 1.6789 - 1s/epoch - 16ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.0674 - val_loss: 1.6688 - 1s/epoch - 17ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.0713 - val_loss: 1.6733 - 1s/epoch - 18ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.0611 - val_loss: 1.7647 - 1s/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.0585 - val_loss: 1.7252 - 1s/epoch - 16ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.0591 - val_loss: 1.7028 - 1s/epoch - 16ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.0654 - val_loss: 1.6900 - 1s/epoch - 16ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.0597 - val_loss: 1.7034 - 1s/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.0550 - val_loss: 1.7053 - 1s/epoch - 16ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.0437 - val_loss: 1.6843 - 1s/epoch - 16ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.0408 - val_loss: 1.7160 - 1s/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.0356 - val_loss: 1.7226 - 1s/epoch - 17ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.0357 - val_loss: 1.7532 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.0504 - val_loss: 1.7323 - 1s/epoch - 16ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.0404 - val_loss: 1.7009 - 1s/epoch - 16ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.0323 - val_loss: 1.6985 - 1s/epoch - 16ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.0307 - val_loss: 1.7167 - 1s/epoch - 16ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0279 - val_loss: 1.7096 - 1s/epoch - 16ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.0284 - val_loss: 1.7129 - 1s/epoch - 16ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.0275 - val_loss: 1.7250 - 1s/epoch - 17ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.0254 - val_loss: 1.7054 - 1s/epoch - 17ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.0260 - val_loss: 1.7263 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.0274 - val_loss: 1.7334 - 1s/epoch - 16ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.0281 - val_loss: 1.7045 - 1s/epoch - 16ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.0274 - val_loss: 1.7128 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.0231 - val_loss: 1.7218 - 1s/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0305 - val_loss: 1.6896 - 1s/epoch - 16ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0299 - val_loss: 1.6831 - 1s/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.0280 - val_loss: 1.6812 - 1s/epoch - 16ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0231 - val_loss: 1.7153 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.0221 - val_loss: 1.7188 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.0217 - val_loss: 1.7350 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.0305 - val_loss: 1.7043 - 1s/epoch - 16ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.0272 - val_loss: 1.6876 - 1s/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0294 - val_loss: 1.6749 - 1s/epoch - 16ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0243 - val_loss: 1.6595 - 1s/epoch - 16ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.0356 - val_loss: 1.6822 - 1s/epoch - 16ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0674 - val_loss: 1.7316 - 1s/epoch - 16ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.0354 - val_loss: 1.6833 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0251 - val_loss: 1.6904 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.0165 - val_loss: 1.6986 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and relu activation function: 1.6985966147239215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 7s - loss: 0.9839 - val_loss: 1.0955 - 7s/epoch - 83ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9765 - val_loss: 1.0973 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9770 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9768 - val_loss: 1.0967 - 1s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9767 - val_loss: 1.0969 - 1s/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9768 - val_loss: 1.0966 - 1s/epoch - 16ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0956 - 1s/epoch - 16ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9764 - val_loss: 1.0957 - 1s/epoch - 16ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9764 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0965 - 1s/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9756 - val_loss: 1.0995 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.9753 - val_loss: 1.0976 - 1s/epoch - 17ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9756 - val_loss: 1.0970 - 1s/epoch - 16ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9749 - val_loss: 1.0952 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9745 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9747 - val_loss: 1.0977 - 1s/epoch - 16ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9737 - val_loss: 1.0967 - 1s/epoch - 16ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9730 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.9732 - val_loss: 1.0966 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9733 - val_loss: 1.0965 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9718 - val_loss: 1.0986 - 1s/epoch - 17ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9719 - val_loss: 1.0980 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9719 - val_loss: 1.0988 - 1s/epoch - 16ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9702 - val_loss: 1.0995 - 1s/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9701 - val_loss: 1.1017 - 1s/epoch - 16ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9698 - val_loss: 1.0975 - 1s/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9687 - val_loss: 1.1025 - 1s/epoch - 16ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.9687 - val_loss: 1.1025 - 1s/epoch - 16ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9699 - val_loss: 1.1014 - 1s/epoch - 17ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.9671 - val_loss: 1.1020 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.9669 - val_loss: 1.1040 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.9668 - val_loss: 1.1026 - 1s/epoch - 16ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.9647 - val_loss: 1.1047 - 1s/epoch - 16ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.9649 - val_loss: 1.1066 - 1s/epoch - 16ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.9630 - val_loss: 1.1067 - 1s/epoch - 16ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.9614 - val_loss: 1.1085 - 1s/epoch - 17ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.9626 - val_loss: 1.1101 - 1s/epoch - 17ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.9601 - val_loss: 1.1139 - 1s/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.9583 - val_loss: 1.1039 - 1s/epoch - 17ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.9548 - val_loss: 1.1126 - 1s/epoch - 17ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.9530 - val_loss: 1.1111 - 1s/epoch - 16ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.9508 - val_loss: 1.1171 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.9496 - val_loss: 1.1216 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.9441 - val_loss: 1.1167 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.9396 - val_loss: 1.1260 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.9365 - val_loss: 1.1315 - 1s/epoch - 18ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.9339 - val_loss: 1.1271 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.9240 - val_loss: 1.1153 - 1s/epoch - 18ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.9139 - val_loss: 1.1204 - 1s/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.9119 - val_loss: 1.1629 - 1s/epoch - 18ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.9023 - val_loss: 1.1441 - 1s/epoch - 17ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.8934 - val_loss: 1.1778 - 1s/epoch - 17ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.8863 - val_loss: 1.1335 - 1s/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.8760 - val_loss: 1.1939 - 1s/epoch - 16ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.8662 - val_loss: 1.1553 - 1s/epoch - 16ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.8828 - val_loss: 1.1720 - 1s/epoch - 17ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.8568 - val_loss: 1.2481 - 1s/epoch - 18ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.8498 - val_loss: 1.2051 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 2s - loss: 0.8383 - val_loss: 1.1817 - 2s/epoch - 18ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.8479 - val_loss: 1.2220 - 1s/epoch - 17ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.8330 - val_loss: 1.1426 - 1s/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.8461 - val_loss: 1.2112 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.8239 - val_loss: 1.1530 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.8172 - val_loss: 1.2021 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.8148 - val_loss: 1.1692 - 1s/epoch - 17ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.8106 - val_loss: 1.3014 - 1s/epoch - 18ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.8084 - val_loss: 1.2163 - 1s/epoch - 18ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.8009 - val_loss: 1.1902 - 1s/epoch - 17ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.7940 - val_loss: 1.1990 - 1s/epoch - 17ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.7950 - val_loss: 1.1903 - 1s/epoch - 17ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.7948 - val_loss: 1.1715 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.7867 - val_loss: 1.2295 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.7832 - val_loss: 1.1885 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.7782 - val_loss: 1.2584 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.7839 - val_loss: 1.2143 - 1s/epoch - 18ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.7724 - val_loss: 1.2021 - 1s/epoch - 17ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.7709 - val_loss: 1.2063 - 1s/epoch - 18ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.7668 - val_loss: 1.2783 - 1s/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 2s - loss: 0.7640 - val_loss: 1.2302 - 2s/epoch - 19ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 2s - loss: 0.7597 - val_loss: 1.3353 - 2s/epoch - 18ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.7756 - val_loss: 1.2431 - 1s/epoch - 18ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.7513 - val_loss: 1.2023 - 1s/epoch - 18ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.7502 - val_loss: 1.1809 - 1s/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.7453 - val_loss: 1.2419 - 1s/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.7405 - val_loss: 1.2175 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.7366 - val_loss: 1.2533 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.7358 - val_loss: 1.2076 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.7268 - val_loss: 1.2229 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.7238 - val_loss: 1.2243 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.7213 - val_loss: 1.2894 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and sigmoid activation function: 1.2893571279908824\n",
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9762 - val_loss: 1.0957 - 6s/epoch - 71ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 0s - loss: 0.9760 - val_loss: 1.0954 - 496ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 0s - loss: 0.9754 - val_loss: 1.0962 - 495ms/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 0s - loss: 0.9749 - val_loss: 1.0961 - 498ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9734 - val_loss: 1.0982 - 504ms/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 0s - loss: 0.9718 - val_loss: 1.0972 - 500ms/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 0s - loss: 0.9697 - val_loss: 1.1015 - 496ms/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 0s - loss: 0.9668 - val_loss: 1.1026 - 500ms/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9622 - val_loss: 1.0979 - 503ms/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 0s - loss: 0.9532 - val_loss: 1.1213 - 496ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9406 - val_loss: 1.1148 - 502ms/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9238 - val_loss: 1.1113 - 501ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9012 - val_loss: 1.1310 - 503ms/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.8800 - val_loss: 1.1433 - 518ms/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.8489 - val_loss: 1.1757 - 548ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.8188 - val_loss: 1.2244 - 562ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.7940 - val_loss: 1.2272 - 538ms/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.7644 - val_loss: 1.2468 - 538ms/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.7461 - val_loss: 1.1995 - 508ms/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.7095 - val_loss: 1.2386 - 506ms/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.6840 - val_loss: 1.2819 - 502ms/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 0s - loss: 0.6567 - val_loss: 1.2935 - 499ms/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.6375 - val_loss: 1.2750 - 501ms/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 0s - loss: 0.6051 - val_loss: 1.3076 - 497ms/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.5775 - val_loss: 1.2844 - 505ms/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.5587 - val_loss: 1.3450 - 503ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.5323 - val_loss: 1.4031 - 511ms/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.5356 - val_loss: 1.4196 - 508ms/epoch - 6ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.4918 - val_loss: 1.4690 - 507ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.4687 - val_loss: 1.4279 - 501ms/epoch - 6ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.4475 - val_loss: 1.4063 - 504ms/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 0s - loss: 0.4285 - val_loss: 1.4409 - 498ms/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.4208 - val_loss: 1.5478 - 507ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 0s - loss: 0.3936 - val_loss: 1.4977 - 499ms/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.3773 - val_loss: 1.5204 - 507ms/epoch - 6ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.3631 - val_loss: 1.5553 - 506ms/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 0s - loss: 0.3381 - val_loss: 1.5871 - 495ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 0s - loss: 0.3221 - val_loss: 1.5771 - 500ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.3158 - val_loss: 1.6168 - 519ms/epoch - 6ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.2951 - val_loss: 1.6137 - 527ms/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.2810 - val_loss: 1.6108 - 526ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.2636 - val_loss: 1.6546 - 521ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.2479 - val_loss: 1.6700 - 516ms/epoch - 6ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 0s - loss: 0.2409 - val_loss: 1.6902 - 492ms/epoch - 6ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 0s - loss: 0.2271 - val_loss: 1.6624 - 494ms/epoch - 6ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 0s - loss: 0.2105 - val_loss: 1.6266 - 489ms/epoch - 6ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 0s - loss: 0.2025 - val_loss: 1.6963 - 492ms/epoch - 6ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 0s - loss: 0.1877 - val_loss: 1.7183 - 497ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 0s - loss: 0.1745 - val_loss: 1.7441 - 497ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 0s - loss: 0.1689 - val_loss: 1.7734 - 493ms/epoch - 6ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 0s - loss: 0.1568 - val_loss: 1.7516 - 491ms/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 0s - loss: 0.1469 - val_loss: 1.7400 - 498ms/epoch - 6ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 0s - loss: 0.1425 - val_loss: 1.8112 - 491ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 0s - loss: 0.1326 - val_loss: 1.7145 - 494ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.1184 - val_loss: 1.7549 - 510ms/epoch - 6ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.1109 - val_loss: 1.7401 - 509ms/epoch - 6ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.1018 - val_loss: 1.8144 - 505ms/epoch - 6ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.1014 - val_loss: 1.7898 - 505ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 0s - loss: 0.0974 - val_loss: 1.8182 - 496ms/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.0822 - val_loss: 1.7971 - 501ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 0s - loss: 0.0742 - val_loss: 1.8261 - 500ms/epoch - 6ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 0s - loss: 0.0703 - val_loss: 1.8254 - 499ms/epoch - 6ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.0685 - val_loss: 1.8083 - 523ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.0605 - val_loss: 1.8079 - 535ms/epoch - 7ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.0616 - val_loss: 1.9156 - 528ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.0629 - val_loss: 1.8427 - 527ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.0650 - val_loss: 1.8828 - 542ms/epoch - 7ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.0515 - val_loss: 1.8153 - 508ms/epoch - 6ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.0445 - val_loss: 1.8437 - 506ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.0412 - val_loss: 1.9032 - 507ms/epoch - 6ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.0497 - val_loss: 1.8158 - 512ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.0391 - val_loss: 1.8301 - 512ms/epoch - 6ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.0403 - val_loss: 1.8772 - 505ms/epoch - 6ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.0309 - val_loss: 1.8676 - 522ms/epoch - 6ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.0287 - val_loss: 1.8992 - 529ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.0292 - val_loss: 1.8985 - 522ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0399 - val_loss: 1.8907 - 503ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.0317 - val_loss: 1.9029 - 501ms/epoch - 6ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.0334 - val_loss: 1.8984 - 507ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.0254 - val_loss: 1.8609 - 508ms/epoch - 6ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.0265 - val_loss: 1.8269 - 509ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.0230 - val_loss: 1.8823 - 506ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.0185 - val_loss: 1.9099 - 507ms/epoch - 6ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 0s - loss: 0.0177 - val_loss: 1.9206 - 499ms/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.0275 - val_loss: 1.8685 - 501ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0232 - val_loss: 1.9301 - 516ms/epoch - 6ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0193 - val_loss: 1.9010 - 540ms/epoch - 7ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.0163 - val_loss: 1.8664 - 537ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0147 - val_loss: 1.8786 - 542ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.0143 - val_loss: 1.9130 - 539ms/epoch - 7ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.0147 - val_loss: 1.8733 - 559ms/epoch - 7ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.0133 - val_loss: 1.8692 - 504ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.0129 - val_loss: 1.8741 - 511ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0156 - val_loss: 1.8750 - 507ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0168 - val_loss: 1.8842 - 507ms/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.0155 - val_loss: 1.9044 - 504ms/epoch - 6ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0171 - val_loss: 1.8933 - 507ms/epoch - 6ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.0189 - val_loss: 1.8652 - 507ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0154 - val_loss: 1.8846 - 503ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.0116 - val_loss: 1.8844 - 503ms/epoch - 6ms/step\n",
            "163/163 [==============================] - 1s 2ms/step\n",
            "Average MSE for the adam optimizer and tanh activation function: 1.8843867844482098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9762 - val_loss: 1.0956 - 6s/epoch - 73ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9756 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9751 - val_loss: 1.0969 - 1s/epoch - 16ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9747 - val_loss: 1.0970 - 1s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9731 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9715 - val_loss: 1.0954 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9709 - val_loss: 1.0956 - 1s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9683 - val_loss: 1.0987 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9629 - val_loss: 1.1197 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9598 - val_loss: 1.1052 - 1s/epoch - 18ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9501 - val_loss: 1.1241 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9399 - val_loss: 1.1303 - 1s/epoch - 16ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9242 - val_loss: 1.1599 - 1s/epoch - 17ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9104 - val_loss: 1.1500 - 1s/epoch - 16ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.8873 - val_loss: 1.1781 - 1s/epoch - 16ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.8683 - val_loss: 1.1588 - 1s/epoch - 17ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.8432 - val_loss: 1.1981 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.8117 - val_loss: 1.1974 - 1s/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.7876 - val_loss: 1.2165 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.7589 - val_loss: 1.1963 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.7310 - val_loss: 1.2323 - 1s/epoch - 16ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.6985 - val_loss: 1.2622 - 1s/epoch - 16ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.6943 - val_loss: 1.2488 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.6670 - val_loss: 1.2503 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.6178 - val_loss: 1.3242 - 1s/epoch - 16ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.5954 - val_loss: 1.2974 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.5850 - val_loss: 1.2750 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.5416 - val_loss: 1.3286 - 1s/epoch - 18ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.5260 - val_loss: 1.3834 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.4929 - val_loss: 1.3866 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.4697 - val_loss: 1.3747 - 1s/epoch - 16ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.4443 - val_loss: 1.4072 - 1s/epoch - 16ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.4268 - val_loss: 1.5296 - 1s/epoch - 16ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.4108 - val_loss: 1.4364 - 1s/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.3769 - val_loss: 1.4599 - 1s/epoch - 16ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.3557 - val_loss: 1.4481 - 1s/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.3396 - val_loss: 1.5551 - 1s/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.3146 - val_loss: 1.4948 - 1s/epoch - 18ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.2985 - val_loss: 1.5547 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.2742 - val_loss: 1.5454 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.2555 - val_loss: 1.6269 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.2435 - val_loss: 1.5839 - 1s/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.2239 - val_loss: 1.5944 - 1s/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.2062 - val_loss: 1.5745 - 1s/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.1912 - val_loss: 1.6200 - 1s/epoch - 18ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.1733 - val_loss: 1.6518 - 1s/epoch - 17ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.1639 - val_loss: 1.6631 - 1s/epoch - 17ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.1515 - val_loss: 1.6427 - 1s/epoch - 16ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.1437 - val_loss: 1.6943 - 1s/epoch - 16ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.1585 - val_loss: 1.6614 - 1s/epoch - 16ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.1316 - val_loss: 1.6352 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.1145 - val_loss: 1.7443 - 1s/epoch - 16ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.1031 - val_loss: 1.6953 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.0978 - val_loss: 1.6925 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.0888 - val_loss: 1.6995 - 1s/epoch - 18ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.0794 - val_loss: 1.7319 - 1s/epoch - 18ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.0736 - val_loss: 1.7562 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.0703 - val_loss: 1.7695 - 1s/epoch - 17ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.0640 - val_loss: 1.7343 - 1s/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.0590 - val_loss: 1.7845 - 1s/epoch - 16ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.0559 - val_loss: 1.7871 - 1s/epoch - 16ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.0502 - val_loss: 1.7946 - 1s/epoch - 17ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.0484 - val_loss: 1.7777 - 1s/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.0471 - val_loss: 1.8052 - 1s/epoch - 17ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.0568 - val_loss: 1.7577 - 1s/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.0658 - val_loss: 1.7399 - 1s/epoch - 16ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.0487 - val_loss: 1.7692 - 1s/epoch - 16ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.0380 - val_loss: 1.7785 - 1s/epoch - 16ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.0349 - val_loss: 1.7737 - 1s/epoch - 16ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.0295 - val_loss: 1.7756 - 1s/epoch - 17ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.0295 - val_loss: 1.8011 - 1s/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.0273 - val_loss: 1.7688 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.0259 - val_loss: 1.7795 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.0246 - val_loss: 1.7917 - 1s/epoch - 18ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.0237 - val_loss: 1.7939 - 1s/epoch - 17ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.0264 - val_loss: 1.7582 - 1s/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0232 - val_loss: 1.7935 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.0331 - val_loss: 1.7991 - 1s/epoch - 16ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.0310 - val_loss: 1.7696 - 1s/epoch - 17ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.0219 - val_loss: 1.7454 - 1s/epoch - 16ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.0221 - val_loss: 1.7824 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.0208 - val_loss: 1.7743 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.0165 - val_loss: 1.7752 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.0151 - val_loss: 1.7753 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.0146 - val_loss: 1.7620 - 1s/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0145 - val_loss: 1.7793 - 1s/epoch - 16ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0155 - val_loss: 1.7856 - 1s/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.0156 - val_loss: 1.7456 - 1s/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0171 - val_loss: 1.7675 - 1s/epoch - 16ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.0410 - val_loss: 1.7394 - 1s/epoch - 16ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.0369 - val_loss: 1.7926 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.0293 - val_loss: 1.7373 - 1s/epoch - 17ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.0220 - val_loss: 1.7337 - 1s/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0152 - val_loss: 1.7365 - 1s/epoch - 16ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0111 - val_loss: 1.7217 - 1s/epoch - 16ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.0088 - val_loss: 1.7278 - 1s/epoch - 16ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0068 - val_loss: 1.7523 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.0069 - val_loss: 1.7328 - 1s/epoch - 16ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0073 - val_loss: 1.7404 - 1s/epoch - 18ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.0068 - val_loss: 1.7541 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and elu activation function: 1.754063754330002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9773 - val_loss: 1.0959 - 6s/epoch - 76ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0954 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9746 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9729 - val_loss: 1.0968 - 1s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9706 - val_loss: 1.0997 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9690 - val_loss: 1.0996 - 1s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9628 - val_loss: 1.1004 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9581 - val_loss: 1.1061 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9458 - val_loss: 1.1253 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9320 - val_loss: 1.1253 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9027 - val_loss: 1.1327 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.8735 - val_loss: 1.1579 - 1s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.8309 - val_loss: 1.1870 - 1s/epoch - 18ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.7813 - val_loss: 1.2473 - 1s/epoch - 18ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.7514 - val_loss: 1.1886 - 1s/epoch - 18ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.7010 - val_loss: 1.2491 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.6614 - val_loss: 1.2528 - 1s/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.6092 - val_loss: 1.3168 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.5510 - val_loss: 1.3716 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.5255 - val_loss: 1.3850 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.4748 - val_loss: 1.3271 - 1s/epoch - 18ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.4311 - val_loss: 1.3960 - 1s/epoch - 18ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.4003 - val_loss: 1.4523 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.3659 - val_loss: 1.4278 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.3308 - val_loss: 1.4703 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.2944 - val_loss: 1.4635 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.3336 - val_loss: 1.3830 - 1s/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.3419 - val_loss: 1.4708 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.2621 - val_loss: 1.5404 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.2145 - val_loss: 1.4970 - 1s/epoch - 18ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.1874 - val_loss: 1.5461 - 1s/epoch - 18ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.1687 - val_loss: 1.5261 - 1s/epoch - 17ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.1555 - val_loss: 1.5738 - 1s/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.1389 - val_loss: 1.5826 - 1s/epoch - 17ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.1424 - val_loss: 1.6040 - 1s/epoch - 18ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.1270 - val_loss: 1.6282 - 1s/epoch - 18ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.1108 - val_loss: 1.6242 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.0932 - val_loss: 1.6447 - 1s/epoch - 17ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 2s - loss: 0.0810 - val_loss: 1.6299 - 2s/epoch - 18ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 2s - loss: 0.0726 - val_loss: 1.6447 - 2s/epoch - 19ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.0826 - val_loss: 1.6657 - 1s/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.0679 - val_loss: 1.6458 - 1s/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.0604 - val_loss: 1.6148 - 1s/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.0534 - val_loss: 1.6394 - 1s/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.0438 - val_loss: 1.6321 - 1s/epoch - 17ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.0413 - val_loss: 1.6834 - 1s/epoch - 16ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.0356 - val_loss: 1.6583 - 1s/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.0306 - val_loss: 1.6893 - 1s/epoch - 18ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.0311 - val_loss: 1.6609 - 1s/epoch - 18ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.0299 - val_loss: 1.6684 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.0282 - val_loss: 1.6547 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.0249 - val_loss: 1.6812 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.0322 - val_loss: 1.6976 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.0312 - val_loss: 1.6533 - 1s/epoch - 18ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.0245 - val_loss: 1.6466 - 1s/epoch - 18ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 2s - loss: 0.0264 - val_loss: 1.6545 - 2s/epoch - 19ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 2s - loss: 0.0269 - val_loss: 1.6421 - 2s/epoch - 20ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 2s - loss: 0.0232 - val_loss: 1.6158 - 2s/epoch - 19ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.0191 - val_loss: 1.6490 - 1s/epoch - 18ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.0241 - val_loss: 1.6417 - 1s/epoch - 18ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.0400 - val_loss: 1.6800 - 1s/epoch - 18ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.0367 - val_loss: 1.6494 - 1s/epoch - 18ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.0248 - val_loss: 1.6608 - 1s/epoch - 18ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.0207 - val_loss: 1.6569 - 1s/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.0164 - val_loss: 1.6461 - 1s/epoch - 18ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.0139 - val_loss: 1.6527 - 1s/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.0194 - val_loss: 1.6318 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.0182 - val_loss: 1.6459 - 1s/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.0146 - val_loss: 1.6578 - 1s/epoch - 17ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.0128 - val_loss: 1.6452 - 1s/epoch - 17ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.0214 - val_loss: 1.6573 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.0220 - val_loss: 1.6315 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.0200 - val_loss: 1.6254 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.0262 - val_loss: 1.6336 - 1s/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.0199 - val_loss: 1.6277 - 1s/epoch - 18ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0156 - val_loss: 1.6131 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.0220 - val_loss: 1.6263 - 1s/epoch - 17ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 2s - loss: 0.0152 - val_loss: 1.6000 - 2s/epoch - 18ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.0140 - val_loss: 1.6005 - 1s/epoch - 17ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.0166 - val_loss: 1.6146 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.0151 - val_loss: 1.5829 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.0180 - val_loss: 1.6224 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.0179 - val_loss: 1.6074 - 1s/epoch - 18ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.0178 - val_loss: 1.5839 - 1s/epoch - 17ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0158 - val_loss: 1.6243 - 1s/epoch - 17ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0155 - val_loss: 1.5922 - 1s/epoch - 17ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.0142 - val_loss: 1.6063 - 1s/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0168 - val_loss: 1.5950 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.0163 - val_loss: 1.5892 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.0151 - val_loss: 1.6143 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.0155 - val_loss: 1.6000 - 1s/epoch - 18ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.0267 - val_loss: 1.5779 - 1s/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0201 - val_loss: 1.5749 - 1s/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0167 - val_loss: 1.5803 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.0146 - val_loss: 1.5666 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0134 - val_loss: 1.5771 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.0109 - val_loss: 1.5852 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0106 - val_loss: 1.5667 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.0174 - val_loss: 1.5843 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and selu activation function: 1.584303399304113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9762 - val_loss: 1.0955 - 6s/epoch - 75ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 20ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 18ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0965 - 2s/epoch - 19ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 2s - loss: 0.9762 - val_loss: 1.0957 - 2s/epoch - 18ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0955 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0965 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0958 - 2s/epoch - 18ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 18ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 18ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 19ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and softmax activation function: 1.0959533066900087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 7s - loss: 0.9761 - val_loss: 1.0960 - 7s/epoch - 80ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0959 - 2s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 20ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 2s - loss: 0.9758 - val_loss: 1.0959 - 2s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 2s - loss: 0.9747 - val_loss: 1.0962 - 2s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 2s - loss: 0.9728 - val_loss: 1.0974 - 2s/epoch - 19ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 2s - loss: 0.9734 - val_loss: 1.0964 - 2s/epoch - 19ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 2s - loss: 0.9706 - val_loss: 1.1001 - 2s/epoch - 19ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 2s - loss: 0.9672 - val_loss: 1.1011 - 2s/epoch - 19ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 2s - loss: 0.9658 - val_loss: 1.1065 - 2s/epoch - 19ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 2s - loss: 0.9615 - val_loss: 1.1016 - 2s/epoch - 20ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 2s - loss: 0.9548 - val_loss: 1.1107 - 2s/epoch - 20ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 2s - loss: 0.9470 - val_loss: 1.1124 - 2s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 2s - loss: 0.9399 - val_loss: 1.1415 - 2s/epoch - 18ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 2s - loss: 0.9337 - val_loss: 1.1138 - 2s/epoch - 19ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 2s - loss: 0.9184 - val_loss: 1.1556 - 2s/epoch - 19ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 2s - loss: 0.9122 - val_loss: 1.1473 - 2s/epoch - 19ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 2s - loss: 0.8939 - val_loss: 1.1766 - 2s/epoch - 19ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 2s - loss: 0.8848 - val_loss: 1.1753 - 2s/epoch - 20ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 2s - loss: 0.8675 - val_loss: 1.1485 - 2s/epoch - 19ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 2s - loss: 0.8468 - val_loss: 1.1648 - 2s/epoch - 19ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 2s - loss: 0.8264 - val_loss: 1.1888 - 2s/epoch - 19ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 2s - loss: 0.8126 - val_loss: 1.1962 - 2s/epoch - 19ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 2s - loss: 0.7847 - val_loss: 1.1775 - 2s/epoch - 19ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 2s - loss: 0.7737 - val_loss: 1.1783 - 2s/epoch - 19ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 2s - loss: 0.7520 - val_loss: 1.2085 - 2s/epoch - 19ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 2s - loss: 0.7313 - val_loss: 1.2422 - 2s/epoch - 20ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 2s - loss: 0.7138 - val_loss: 1.2294 - 2s/epoch - 20ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 2s - loss: 0.6991 - val_loss: 1.2625 - 2s/epoch - 19ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 2s - loss: 0.6772 - val_loss: 1.2533 - 2s/epoch - 19ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 2s - loss: 0.6568 - val_loss: 1.3320 - 2s/epoch - 19ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 2s - loss: 0.6452 - val_loss: 1.2767 - 2s/epoch - 19ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 2s - loss: 0.6191 - val_loss: 1.3196 - 2s/epoch - 19ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 2s - loss: 0.6073 - val_loss: 1.3370 - 2s/epoch - 19ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 2s - loss: 0.5887 - val_loss: 1.3797 - 2s/epoch - 20ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 2s - loss: 0.5770 - val_loss: 1.3618 - 2s/epoch - 20ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 2s - loss: 0.5509 - val_loss: 1.3872 - 2s/epoch - 19ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 2s - loss: 0.5374 - val_loss: 1.4486 - 2s/epoch - 19ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 2s - loss: 0.5116 - val_loss: 1.4694 - 2s/epoch - 19ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 2s - loss: 0.4952 - val_loss: 1.4473 - 2s/epoch - 19ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 2s - loss: 0.4777 - val_loss: 1.5844 - 2s/epoch - 19ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 2s - loss: 0.4776 - val_loss: 1.4112 - 2s/epoch - 19ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 2s - loss: 0.4438 - val_loss: 1.4687 - 2s/epoch - 20ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 2s - loss: 0.4222 - val_loss: 1.4899 - 2s/epoch - 20ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 2s - loss: 0.3991 - val_loss: 1.5382 - 2s/epoch - 19ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 2s - loss: 0.3839 - val_loss: 1.4930 - 2s/epoch - 19ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 2s - loss: 0.3676 - val_loss: 1.4767 - 2s/epoch - 19ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 2s - loss: 0.3491 - val_loss: 1.4567 - 2s/epoch - 19ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 2s - loss: 0.3383 - val_loss: 1.5839 - 2s/epoch - 19ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 2s - loss: 0.3207 - val_loss: 1.5984 - 2s/epoch - 19ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 2s - loss: 0.3011 - val_loss: 1.5475 - 2s/epoch - 20ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 2s - loss: 0.2832 - val_loss: 1.6424 - 2s/epoch - 19ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.2802 - val_loss: 1.6203 - 1s/epoch - 18ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 2s - loss: 0.2593 - val_loss: 1.6849 - 2s/epoch - 19ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 2s - loss: 0.2475 - val_loss: 1.7050 - 2s/epoch - 19ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 2s - loss: 0.2304 - val_loss: 1.6957 - 2s/epoch - 19ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 2s - loss: 0.2267 - val_loss: 1.7153 - 2s/epoch - 19ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 2s - loss: 0.2075 - val_loss: 1.5681 - 2s/epoch - 20ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 2s - loss: 0.1985 - val_loss: 1.6927 - 2s/epoch - 20ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 2s - loss: 0.1868 - val_loss: 1.6951 - 2s/epoch - 20ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 2s - loss: 0.1793 - val_loss: 1.7936 - 2s/epoch - 19ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 2s - loss: 0.1771 - val_loss: 1.6520 - 2s/epoch - 19ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 2s - loss: 0.1538 - val_loss: 1.7321 - 2s/epoch - 19ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 2s - loss: 0.1421 - val_loss: 1.7489 - 2s/epoch - 18ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 2s - loss: 0.1357 - val_loss: 1.7206 - 2s/epoch - 19ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 2s - loss: 0.1314 - val_loss: 1.7252 - 2s/epoch - 19ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 2s - loss: 0.1194 - val_loss: 1.6996 - 2s/epoch - 19ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 2s - loss: 0.1157 - val_loss: 1.7636 - 2s/epoch - 19ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 2s - loss: 0.1128 - val_loss: 1.8113 - 2s/epoch - 18ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 2s - loss: 0.1066 - val_loss: 1.7661 - 2s/epoch - 19ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 2s - loss: 0.3308 - val_loss: 1.5958 - 2s/epoch - 19ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 2s - loss: 0.1745 - val_loss: 1.6888 - 2s/epoch - 19ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 2s - loss: 0.1260 - val_loss: 1.7080 - 2s/epoch - 19ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 2s - loss: 0.1023 - val_loss: 1.7303 - 2s/epoch - 19ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 2s - loss: 0.1023 - val_loss: 1.7184 - 2s/epoch - 19ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 2s - loss: 0.0854 - val_loss: 1.7339 - 2s/epoch - 19ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0776 - val_loss: 1.7717 - 1s/epoch - 18ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 2s - loss: 0.0713 - val_loss: 1.7639 - 2s/epoch - 18ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.0686 - val_loss: 1.7244 - 1s/epoch - 18ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 2s - loss: 0.0602 - val_loss: 1.7588 - 2s/epoch - 18ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 2s - loss: 0.0562 - val_loss: 1.7621 - 2s/epoch - 18ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 2s - loss: 0.0522 - val_loss: 1.7830 - 2s/epoch - 19ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 2s - loss: 0.0522 - val_loss: 1.8112 - 2s/epoch - 19ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 2s - loss: 0.0510 - val_loss: 1.7924 - 2s/epoch - 19ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 2s - loss: 0.0466 - val_loss: 1.7841 - 2s/epoch - 19ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0447 - val_loss: 1.7778 - 1s/epoch - 18ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0457 - val_loss: 1.7836 - 1s/epoch - 18ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 2s - loss: 0.0448 - val_loss: 1.7833 - 2s/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0426 - val_loss: 1.8291 - 1s/epoch - 18ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 2s - loss: 0.0374 - val_loss: 1.8102 - 2s/epoch - 18ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 2s - loss: 0.0334 - val_loss: 1.8055 - 2s/epoch - 19ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 2s - loss: 0.0320 - val_loss: 1.7625 - 2s/epoch - 19ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 2s - loss: 0.0725 - val_loss: 1.7652 - 2s/epoch - 19ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0464 - val_loss: 1.7991 - 1s/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0343 - val_loss: 1.7484 - 1s/epoch - 18ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 2s - loss: 0.0283 - val_loss: 1.7612 - 2s/epoch - 19ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0282 - val_loss: 1.7792 - 1s/epoch - 18ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 2s - loss: 0.0263 - val_loss: 1.7667 - 2s/epoch - 18ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0308 - val_loss: 1.7626 - 1s/epoch - 18ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 2s - loss: 0.0242 - val_loss: 1.7861 - 2s/epoch - 19ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the adam optimizer and swish activation function: 1.7860286924905349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9761 - val_loss: 1.0973 - 6s/epoch - 78ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 16ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0971 - 1s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9758 - val_loss: 1.0966 - 1s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9759 - val_loss: 1.0953 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9759 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9757 - val_loss: 1.0951 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9756 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9754 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9754 - val_loss: 1.0952 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9752 - val_loss: 1.0968 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9750 - val_loss: 1.0970 - 1s/epoch - 16ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9742 - val_loss: 1.1010 - 1s/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9738 - val_loss: 1.0967 - 1s/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9725 - val_loss: 1.0956 - 1s/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9723 - val_loss: 1.0979 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9711 - val_loss: 1.0992 - 1s/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9693 - val_loss: 1.1082 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9681 - val_loss: 1.1047 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9662 - val_loss: 1.1129 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.9648 - val_loss: 1.1286 - 1s/epoch - 17ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9632 - val_loss: 1.1068 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9595 - val_loss: 1.1101 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9588 - val_loss: 1.1103 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9547 - val_loss: 1.1169 - 1s/epoch - 16ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9510 - val_loss: 1.1456 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9484 - val_loss: 1.1213 - 1s/epoch - 16ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.9450 - val_loss: 1.1325 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9416 - val_loss: 1.1333 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9378 - val_loss: 1.1779 - 1s/epoch - 16ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9314 - val_loss: 1.1593 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9257 - val_loss: 1.1737 - 1s/epoch - 17ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9199 - val_loss: 1.1892 - 1s/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9126 - val_loss: 1.2389 - 1s/epoch - 16ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9078 - val_loss: 1.1471 - 1s/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.8997 - val_loss: 1.1458 - 1s/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.8860 - val_loss: 1.1944 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.8921 - val_loss: 1.2602 - 1s/epoch - 17ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.8704 - val_loss: 1.2013 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.8664 - val_loss: 1.6707 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.8560 - val_loss: 1.1720 - 1s/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.8428 - val_loss: 1.1876 - 1s/epoch - 16ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.8368 - val_loss: 1.1827 - 1s/epoch - 16ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.8151 - val_loss: 2.6470 - 1s/epoch - 16ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.8105 - val_loss: 1.2460 - 1s/epoch - 17ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.7980 - val_loss: 1.1904 - 1s/epoch - 16ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.7837 - val_loss: 1.3220 - 1s/epoch - 16ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.7753 - val_loss: 1.2088 - 1s/epoch - 16ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.7759 - val_loss: 1.2025 - 1s/epoch - 18ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.7657 - val_loss: 1.2095 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.7869 - val_loss: 1.3225 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.7313 - val_loss: 1.4689 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.7283 - val_loss: 1.2508 - 1s/epoch - 16ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.7242 - val_loss: 1.1566 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.6985 - val_loss: 1.3173 - 1s/epoch - 17ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.7066 - val_loss: 1.6353 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.7054 - val_loss: 1.1904 - 1s/epoch - 16ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.6810 - val_loss: 2.4877 - 1s/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.6874 - val_loss: 1.2937 - 1s/epoch - 17ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.6778 - val_loss: 1.2450 - 1s/epoch - 16ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.6528 - val_loss: 1.3602 - 1s/epoch - 16ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.6613 - val_loss: 1.2411 - 1s/epoch - 16ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.6356 - val_loss: 1.2574 - 1s/epoch - 17ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.6282 - val_loss: 1.2647 - 1s/epoch - 17ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.6331 - val_loss: 1.2896 - 1s/epoch - 16ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.6148 - val_loss: 1.2567 - 1s/epoch - 16ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.6096 - val_loss: 1.8608 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.6064 - val_loss: 1.3454 - 1s/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.5902 - val_loss: 1.3988 - 1s/epoch - 17ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.5802 - val_loss: 1.3012 - 1s/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.5778 - val_loss: 1.2837 - 1s/epoch - 16ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.5657 - val_loss: 1.4578 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.5553 - val_loss: 1.4209 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.5467 - val_loss: 1.3144 - 1s/epoch - 16ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.5382 - val_loss: 1.2899 - 1s/epoch - 16ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.5429 - val_loss: 1.9504 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.5268 - val_loss: 1.2844 - 1s/epoch - 18ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.5170 - val_loss: 1.3375 - 1s/epoch - 18ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.5088 - val_loss: 1.8949 - 1s/epoch - 16ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.4984 - val_loss: 1.4125 - 1s/epoch - 16ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.4906 - val_loss: 1.3390 - 1s/epoch - 16ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.4767 - val_loss: 1.4547 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.4765 - val_loss: 1.2529 - 1s/epoch - 16ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.4632 - val_loss: 1.5011 - 1s/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.4632 - val_loss: 1.4570 - 1s/epoch - 16ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.4459 - val_loss: 1.3823 - 1s/epoch - 17ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.4458 - val_loss: 1.7290 - 1s/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.4341 - val_loss: 1.2938 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.4266 - val_loss: 1.3983 - 1s/epoch - 16ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.4120 - val_loss: 1.4354 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.4064 - val_loss: 1.4365 - 1s/epoch - 17ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.3947 - val_loss: 2.4594 - 1s/epoch - 16ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.3914 - val_loss: 1.6420 - 1s/epoch - 16ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.3858 - val_loss: 1.5415 - 1s/epoch - 16ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.3744 - val_loss: 1.6472 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.3627 - val_loss: 1.5623 - 1s/epoch - 18ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.3578 - val_loss: 1.5597 - 1s/epoch - 16ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.3461 - val_loss: 1.4054 - 1s/epoch - 16ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.3455 - val_loss: 1.5677 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the rmsprop optimizer and relu activation function: 1.567491279898536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 5s - loss: 0.9829 - val_loss: 1.1209 - 5s/epoch - 67ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9788 - val_loss: 1.1122 - 1s/epoch - 17ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9779 - val_loss: 1.1188 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9775 - val_loss: 1.0988 - 1s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9771 - val_loss: 1.1001 - 1s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9767 - val_loss: 1.0972 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9768 - val_loss: 1.0999 - 1s/epoch - 16ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9766 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9765 - val_loss: 1.0975 - 1s/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9764 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.1008 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9764 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.0995 - 1s/epoch - 16ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.0971 - 1s/epoch - 16ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 16ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0965 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0966 - 1s/epoch - 16ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0972 - 1s/epoch - 16ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0969 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0976 - 1s/epoch - 16ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 16ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0972 - 1s/epoch - 16ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0966 - 1s/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0971 - 1s/epoch - 16ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0977 - 1s/epoch - 16ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0977 - 1s/epoch - 16ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0966 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0968 - 1s/epoch - 16ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 16ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 16ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 16ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0969 - 1s/epoch - 16ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 16ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0974 - 1s/epoch - 16ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0965 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 16ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0965 - 1s/epoch - 16ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0966 - 1s/epoch - 16ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 16ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 16ms/step\n",
            "163/163 [==============================] - 1s 2ms/step\n",
            "Average MSE for the rmsprop optimizer and sigmoid activation function: 1.096217260216236\n",
            "Epoch 1/100\n",
            "82/82 - 5s - loss: 0.9762 - val_loss: 1.0959 - 5s/epoch - 67ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 0s - loss: 0.9760 - val_loss: 1.0953 - 492ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 0s - loss: 0.9760 - val_loss: 1.0954 - 487ms/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 0s - loss: 0.9759 - val_loss: 1.0956 - 481ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 0s - loss: 0.9757 - val_loss: 1.0954 - 484ms/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 0s - loss: 0.9756 - val_loss: 1.0969 - 483ms/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 0s - loss: 0.9754 - val_loss: 1.0956 - 499ms/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 0s - loss: 0.9750 - val_loss: 1.0977 - 498ms/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 0s - loss: 0.9749 - val_loss: 1.1001 - 490ms/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 0s - loss: 0.9742 - val_loss: 1.0980 - 487ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 0s - loss: 0.9738 - val_loss: 1.0982 - 489ms/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 0s - loss: 0.9735 - val_loss: 1.0979 - 487ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 0s - loss: 0.9727 - val_loss: 1.1008 - 484ms/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 0s - loss: 0.9720 - val_loss: 1.1075 - 488ms/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 0s - loss: 0.9721 - val_loss: 1.0979 - 487ms/epoch - 6ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9712 - val_loss: 1.0979 - 512ms/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9705 - val_loss: 1.1075 - 511ms/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9701 - val_loss: 1.1071 - 510ms/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9686 - val_loss: 1.1007 - 528ms/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9677 - val_loss: 1.1064 - 517ms/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 0s - loss: 0.9666 - val_loss: 1.1032 - 491ms/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 0s - loss: 0.9655 - val_loss: 1.1092 - 483ms/epoch - 6ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 0s - loss: 0.9650 - val_loss: 1.1111 - 485ms/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 0s - loss: 0.9626 - val_loss: 1.1195 - 481ms/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 0s - loss: 0.9614 - val_loss: 1.1011 - 479ms/epoch - 6ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 0s - loss: 0.9598 - val_loss: 1.1223 - 483ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 0s - loss: 0.9576 - val_loss: 1.1123 - 483ms/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 0s - loss: 0.9558 - val_loss: 1.1064 - 493ms/epoch - 6ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 0s - loss: 0.9536 - val_loss: 1.1066 - 486ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 0s - loss: 0.9500 - val_loss: 1.1159 - 486ms/epoch - 6ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 0s - loss: 0.9478 - val_loss: 1.1380 - 483ms/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 0s - loss: 0.9434 - val_loss: 1.1410 - 482ms/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 0s - loss: 0.9400 - val_loss: 1.1630 - 480ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 0s - loss: 0.9355 - val_loss: 1.1517 - 485ms/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 0s - loss: 0.9324 - val_loss: 1.1417 - 480ms/epoch - 6ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 0s - loss: 0.9260 - val_loss: 1.1352 - 480ms/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9215 - val_loss: 1.1565 - 513ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 0s - loss: 0.9143 - val_loss: 1.1802 - 493ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 0s - loss: 0.9061 - val_loss: 1.2295 - 488ms/epoch - 6ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 0s - loss: 0.9005 - val_loss: 1.2021 - 488ms/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.8924 - val_loss: 1.2079 - 510ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.8830 - val_loss: 1.1775 - 517ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.8731 - val_loss: 1.4718 - 515ms/epoch - 6ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.8595 - val_loss: 1.1747 - 519ms/epoch - 6ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.8452 - val_loss: 1.3409 - 524ms/epoch - 6ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 0s - loss: 0.8338 - val_loss: 1.2380 - 493ms/epoch - 6ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 0s - loss: 0.8190 - val_loss: 1.1922 - 489ms/epoch - 6ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 0s - loss: 0.8017 - val_loss: 1.2823 - 486ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 0s - loss: 0.7914 - val_loss: 1.2477 - 484ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 0s - loss: 0.7733 - val_loss: 1.3185 - 483ms/epoch - 6ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.7584 - val_loss: 1.1832 - 504ms/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 0s - loss: 0.7460 - val_loss: 1.2627 - 496ms/epoch - 6ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.7337 - val_loss: 1.2672 - 509ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 0s - loss: 0.7233 - val_loss: 1.3247 - 479ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 0s - loss: 0.7052 - val_loss: 1.2764 - 488ms/epoch - 6ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 0s - loss: 0.6891 - val_loss: 1.3222 - 478ms/epoch - 6ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 0s - loss: 0.6772 - val_loss: 1.6523 - 486ms/epoch - 6ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 0s - loss: 0.6670 - val_loss: 1.3133 - 485ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 0s - loss: 0.6551 - val_loss: 1.2766 - 492ms/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 0s - loss: 0.6368 - val_loss: 1.3114 - 484ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 0s - loss: 0.6249 - val_loss: 1.2878 - 489ms/epoch - 6ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 0s - loss: 0.6148 - val_loss: 1.6832 - 480ms/epoch - 6ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 0s - loss: 0.6003 - val_loss: 1.4018 - 489ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 0s - loss: 0.5930 - val_loss: 1.3963 - 480ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 0s - loss: 0.5775 - val_loss: 1.2970 - 484ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.5673 - val_loss: 1.6187 - 513ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.5570 - val_loss: 1.3279 - 521ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.5449 - val_loss: 1.4556 - 525ms/epoch - 6ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.5332 - val_loss: 1.6012 - 550ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.5248 - val_loss: 1.3169 - 543ms/epoch - 7ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.5084 - val_loss: 1.5463 - 501ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 0s - loss: 0.5073 - val_loss: 1.3950 - 484ms/epoch - 6ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 0s - loss: 0.4950 - val_loss: 2.0542 - 485ms/epoch - 6ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 0s - loss: 0.4845 - val_loss: 1.4478 - 481ms/epoch - 6ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 0s - loss: 0.4803 - val_loss: 1.3094 - 485ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 0s - loss: 0.4667 - val_loss: 1.4175 - 477ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 0s - loss: 0.4610 - val_loss: 1.4973 - 484ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 0s - loss: 0.4567 - val_loss: 1.5310 - 477ms/epoch - 6ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 0s - loss: 0.4426 - val_loss: 1.6253 - 486ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 0s - loss: 0.4382 - val_loss: 1.4630 - 487ms/epoch - 6ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 0s - loss: 0.4258 - val_loss: 1.4481 - 480ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 0s - loss: 0.4191 - val_loss: 1.7247 - 475ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 0s - loss: 0.4128 - val_loss: 1.8571 - 481ms/epoch - 6ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 0s - loss: 0.4049 - val_loss: 1.5634 - 478ms/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 0s - loss: 0.3974 - val_loss: 1.4212 - 479ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 0s - loss: 0.3892 - val_loss: 1.6633 - 476ms/epoch - 6ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 0s - loss: 0.3815 - val_loss: 1.8226 - 474ms/epoch - 6ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 0s - loss: 0.3745 - val_loss: 1.5067 - 482ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 0s - loss: 0.3661 - val_loss: 1.4183 - 473ms/epoch - 6ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 0s - loss: 0.3627 - val_loss: 1.7541 - 479ms/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 0s - loss: 0.3543 - val_loss: 1.6390 - 497ms/epoch - 6ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.3471 - val_loss: 1.4907 - 513ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.3426 - val_loss: 1.6214 - 518ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.3351 - val_loss: 2.1610 - 528ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.3298 - val_loss: 1.5204 - 530ms/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.3220 - val_loss: 1.5652 - 503ms/epoch - 6ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 0s - loss: 0.3188 - val_loss: 1.6847 - 482ms/epoch - 6ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 0s - loss: 0.3115 - val_loss: 1.6143 - 485ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 0s - loss: 0.3064 - val_loss: 1.5812 - 489ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 0s - loss: 0.2978 - val_loss: 1.6997 - 499ms/epoch - 6ms/step\n",
            "163/163 [==============================] - 1s 2ms/step\n",
            "Average MSE for the rmsprop optimizer and tanh activation function: 1.6995887848253155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9763 - val_loss: 1.0955 - 6s/epoch - 69ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0953 - 1s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9759 - val_loss: 1.0953 - 1s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9757 - val_loss: 1.0957 - 1s/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9755 - val_loss: 1.0981 - 1s/epoch - 17ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9754 - val_loss: 1.0974 - 1s/epoch - 16ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9751 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9746 - val_loss: 1.0983 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9741 - val_loss: 1.0956 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9730 - val_loss: 1.0968 - 1s/epoch - 18ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9731 - val_loss: 1.0987 - 1s/epoch - 17ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9725 - val_loss: 1.1054 - 1s/epoch - 16ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9719 - val_loss: 1.0988 - 1s/epoch - 16ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9710 - val_loss: 1.0991 - 1s/epoch - 16ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9705 - val_loss: 1.1033 - 1s/epoch - 17ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9693 - val_loss: 1.1000 - 1s/epoch - 16ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9694 - val_loss: 1.0974 - 1s/epoch - 16ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9685 - val_loss: 1.1026 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9674 - val_loss: 1.1001 - 1s/epoch - 18ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9666 - val_loss: 1.1022 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.9655 - val_loss: 1.1031 - 1s/epoch - 17ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9644 - val_loss: 1.1130 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9642 - val_loss: 1.1021 - 1s/epoch - 16ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9619 - val_loss: 1.1064 - 1s/epoch - 16ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9596 - val_loss: 1.1100 - 1s/epoch - 16ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9584 - val_loss: 1.1087 - 1s/epoch - 16ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9565 - val_loss: 1.1131 - 1s/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 2s - loss: 0.9539 - val_loss: 1.1450 - 2s/epoch - 19ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9513 - val_loss: 1.1191 - 1s/epoch - 17ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9494 - val_loss: 1.1350 - 1s/epoch - 17ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9459 - val_loss: 1.1474 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9422 - val_loss: 1.1114 - 1s/epoch - 17ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9376 - val_loss: 1.1264 - 1s/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9345 - val_loss: 1.1414 - 1s/epoch - 16ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9298 - val_loss: 1.1419 - 1s/epoch - 17ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9259 - val_loss: 1.1589 - 1s/epoch - 18ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.9200 - val_loss: 1.1540 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9134 - val_loss: 1.1743 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.9072 - val_loss: 1.2329 - 1s/epoch - 16ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.8983 - val_loss: 1.2015 - 1s/epoch - 16ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.8932 - val_loss: 1.1629 - 1s/epoch - 16ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.8821 - val_loss: 1.1903 - 1s/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.8725 - val_loss: 1.1692 - 1s/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.8652 - val_loss: 1.1939 - 1s/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.8545 - val_loss: 1.1627 - 1s/epoch - 16ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.8473 - val_loss: 1.1929 - 1s/epoch - 17ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.8296 - val_loss: 1.2872 - 1s/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.8196 - val_loss: 1.2069 - 1s/epoch - 16ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.8119 - val_loss: 1.2282 - 1s/epoch - 16ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.8018 - val_loss: 1.1958 - 1s/epoch - 16ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.7836 - val_loss: 1.2214 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.7745 - val_loss: 1.2585 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.7669 - val_loss: 1.8372 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.7562 - val_loss: 1.4406 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.7437 - val_loss: 1.4160 - 1s/epoch - 17ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.7347 - val_loss: 1.5956 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.7172 - val_loss: 1.8282 - 1s/epoch - 17ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.7080 - val_loss: 1.3481 - 1s/epoch - 16ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.6962 - val_loss: 1.2777 - 1s/epoch - 17ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.6810 - val_loss: 1.2575 - 1s/epoch - 16ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.6704 - val_loss: 1.2739 - 1s/epoch - 16ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.6611 - val_loss: 1.2554 - 1s/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.6510 - val_loss: 1.4922 - 1s/epoch - 17ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.6388 - val_loss: 1.3471 - 1s/epoch - 17ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.6311 - val_loss: 1.4468 - 1s/epoch - 18ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.6157 - val_loss: 1.3460 - 1s/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.6005 - val_loss: 1.2551 - 1s/epoch - 16ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.5976 - val_loss: 1.3522 - 1s/epoch - 16ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.5818 - val_loss: 2.2202 - 1s/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.5771 - val_loss: 1.6034 - 1s/epoch - 16ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.5641 - val_loss: 1.7949 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.5503 - val_loss: 1.5500 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.5416 - val_loss: 1.6887 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.5283 - val_loss: 1.4060 - 1s/epoch - 17ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.5218 - val_loss: 1.3432 - 1s/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.5098 - val_loss: 1.4932 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.4984 - val_loss: 1.5971 - 1s/epoch - 16ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.4887 - val_loss: 1.4141 - 1s/epoch - 16ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.4785 - val_loss: 1.4140 - 1s/epoch - 16ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.4625 - val_loss: 1.2923 - 1s/epoch - 16ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.4579 - val_loss: 1.6169 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.4477 - val_loss: 1.4269 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.4331 - val_loss: 1.6003 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.4220 - val_loss: 1.5849 - 1s/epoch - 17ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.4131 - val_loss: 1.6853 - 1s/epoch - 16ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.4046 - val_loss: 1.4962 - 1s/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.3950 - val_loss: 1.4538 - 1s/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.3868 - val_loss: 1.6525 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.3747 - val_loss: 1.6155 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.3642 - val_loss: 1.4719 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.3551 - val_loss: 1.5897 - 1s/epoch - 16ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.3457 - val_loss: 1.5486 - 1s/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.3370 - val_loss: 1.4898 - 1s/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.3265 - val_loss: 1.5219 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.3206 - val_loss: 1.6634 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.3085 - val_loss: 1.6267 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.3011 - val_loss: 1.7197 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.2916 - val_loss: 1.6059 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.2835 - val_loss: 1.6673 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the rmsprop optimizer and elu activation function: 1.667131366955679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 5s - loss: 0.9776 - val_loss: 1.0961 - 5s/epoch - 66ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9763 - val_loss: 1.0956 - 1s/epoch - 16ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9753 - val_loss: 1.0985 - 1s/epoch - 16ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9748 - val_loss: 1.0954 - 1s/epoch - 16ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9736 - val_loss: 1.0971 - 1s/epoch - 16ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9730 - val_loss: 1.0980 - 1s/epoch - 16ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9715 - val_loss: 1.1045 - 1s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9700 - val_loss: 1.0982 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9678 - val_loss: 1.1044 - 1s/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9669 - val_loss: 1.1028 - 1s/epoch - 16ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9646 - val_loss: 1.1151 - 1s/epoch - 16ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9628 - val_loss: 1.0996 - 1s/epoch - 16ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9588 - val_loss: 1.0997 - 1s/epoch - 17ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9558 - val_loss: 1.1097 - 1s/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9512 - val_loss: 1.1607 - 1s/epoch - 16ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9449 - val_loss: 1.1587 - 1s/epoch - 17ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9368 - val_loss: 1.1547 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9282 - val_loss: 1.2414 - 1s/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9190 - val_loss: 1.1463 - 1s/epoch - 16ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9073 - val_loss: 1.1727 - 1s/epoch - 16ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.8907 - val_loss: 1.2424 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 1s - loss: 0.8760 - val_loss: 1.3318 - 1s/epoch - 16ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.8614 - val_loss: 1.3105 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.8451 - val_loss: 1.1924 - 1s/epoch - 16ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.8173 - val_loss: 1.3096 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.7950 - val_loss: 1.2954 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.7705 - val_loss: 1.5182 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.7428 - val_loss: 1.6695 - 1s/epoch - 16ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.7178 - val_loss: 1.3202 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.6942 - val_loss: 1.4322 - 1s/epoch - 16ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.6657 - val_loss: 1.3212 - 1s/epoch - 17ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.6402 - val_loss: 1.3456 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.6119 - val_loss: 1.3643 - 1s/epoch - 16ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.5870 - val_loss: 1.4019 - 1s/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.5605 - val_loss: 1.4549 - 1s/epoch - 17ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.5431 - val_loss: 1.3664 - 1s/epoch - 17ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.5175 - val_loss: 1.3986 - 1s/epoch - 16ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.4919 - val_loss: 1.3928 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.4717 - val_loss: 1.4431 - 1s/epoch - 17ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.4507 - val_loss: 1.4574 - 1s/epoch - 17ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.4294 - val_loss: 1.4714 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.4095 - val_loss: 1.4197 - 1s/epoch - 16ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.3928 - val_loss: 1.5018 - 1s/epoch - 16ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.3714 - val_loss: 1.5190 - 1s/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.3540 - val_loss: 1.6383 - 1s/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.3368 - val_loss: 1.7382 - 1s/epoch - 16ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.3211 - val_loss: 1.5982 - 1s/epoch - 16ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.2994 - val_loss: 1.7580 - 1s/epoch - 17ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.2911 - val_loss: 1.8610 - 1s/epoch - 17ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.2758 - val_loss: 1.5627 - 1s/epoch - 17ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.2591 - val_loss: 1.6556 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.2479 - val_loss: 1.5654 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.2344 - val_loss: 1.5911 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.2218 - val_loss: 1.5658 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.2115 - val_loss: 1.6983 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.1974 - val_loss: 1.5042 - 1s/epoch - 16ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.1858 - val_loss: 1.8132 - 1s/epoch - 17ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.1814 - val_loss: 1.6585 - 1s/epoch - 18ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.1692 - val_loss: 1.5722 - 1s/epoch - 16ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.1585 - val_loss: 1.6044 - 1s/epoch - 16ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.1512 - val_loss: 1.6251 - 1s/epoch - 16ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.1448 - val_loss: 1.5664 - 1s/epoch - 17ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.1362 - val_loss: 1.7383 - 1s/epoch - 17ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.1307 - val_loss: 1.5700 - 1s/epoch - 16ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.1249 - val_loss: 1.7665 - 1s/epoch - 16ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.1165 - val_loss: 1.7346 - 1s/epoch - 16ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.1139 - val_loss: 1.5673 - 1s/epoch - 17ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.1052 - val_loss: 1.6617 - 1s/epoch - 16ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.1042 - val_loss: 1.6942 - 1s/epoch - 16ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.0949 - val_loss: 1.6251 - 1s/epoch - 16ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.0941 - val_loss: 1.6108 - 1s/epoch - 17ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.0874 - val_loss: 1.6004 - 1s/epoch - 18ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.0852 - val_loss: 1.6850 - 1s/epoch - 16ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.0809 - val_loss: 1.6402 - 1s/epoch - 16ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.0771 - val_loss: 1.6780 - 1s/epoch - 16ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.0744 - val_loss: 1.7410 - 1s/epoch - 16ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.0702 - val_loss: 1.6877 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.0700 - val_loss: 1.6245 - 1s/epoch - 16ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.0668 - val_loss: 1.6770 - 1s/epoch - 16ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.0629 - val_loss: 1.6275 - 1s/epoch - 18ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.0603 - val_loss: 1.7763 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.0589 - val_loss: 1.5975 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.0563 - val_loss: 1.6319 - 1s/epoch - 17ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.0554 - val_loss: 1.7272 - 1s/epoch - 17ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.0520 - val_loss: 1.6254 - 1s/epoch - 16ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.0519 - val_loss: 1.6815 - 1s/epoch - 16ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.0498 - val_loss: 1.6224 - 1s/epoch - 16ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.0471 - val_loss: 1.6518 - 1s/epoch - 16ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.0490 - val_loss: 1.6043 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.0473 - val_loss: 1.6354 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.0444 - val_loss: 1.6434 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.0417 - val_loss: 1.6522 - 1s/epoch - 16ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.0422 - val_loss: 1.6310 - 1s/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.0416 - val_loss: 1.5688 - 1s/epoch - 18ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.0390 - val_loss: 1.5437 - 1s/epoch - 16ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.0380 - val_loss: 1.6103 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.0379 - val_loss: 1.6534 - 1s/epoch - 16ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.0376 - val_loss: 1.6248 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.0357 - val_loss: 1.7359 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.0371 - val_loss: 1.6488 - 1s/epoch - 16ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the rmsprop optimizer and selu activation function: 1.6487379253169203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 6s - loss: 0.9761 - val_loss: 1.0954 - 6s/epoch - 68ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0966 - 1s/epoch - 18ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0966 - 1s/epoch - 18ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0954 - 1s/epoch - 17ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9762 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0956 - 1s/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0954 - 1s/epoch - 18ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0956 - 1s/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0970 - 1s/epoch - 16ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 18ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 17ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0956 - 1s/epoch - 18ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 18ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 17ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0966 - 1s/epoch - 17ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0965 - 1s/epoch - 17ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 17ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0970 - 1s/epoch - 17ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0965 - 1s/epoch - 18ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 17ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 17ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 17ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 17ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 17ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the rmsprop optimizer and softmax activation function: 1.0960054157654937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "82/82 - 7s - loss: 0.9761 - val_loss: 1.0958 - 7s/epoch - 89ms/step\n",
            "Epoch 2/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0955 - 2s/epoch - 19ms/step\n",
            "Epoch 3/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 4/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0968 - 2s/epoch - 20ms/step\n",
            "Epoch 5/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0974 - 2s/epoch - 19ms/step\n",
            "Epoch 6/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 7/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 8/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0964 - 1s/epoch - 18ms/step\n",
            "Epoch 9/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 10/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 11/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 12/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0969 - 2s/epoch - 19ms/step\n",
            "Epoch 13/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0964 - 2s/epoch - 19ms/step\n",
            "Epoch 14/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0955 - 2s/epoch - 19ms/step\n",
            "Epoch 15/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0954 - 1s/epoch - 18ms/step\n",
            "Epoch 16/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 17/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 18/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 19/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 18ms/step\n",
            "Epoch 20/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 18ms/step\n",
            "Epoch 21/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0961 - 2s/epoch - 20ms/step\n",
            "Epoch 22/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0962 - 2s/epoch - 19ms/step\n",
            "Epoch 23/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 24/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 25/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 26/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 27/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 18ms/step\n",
            "Epoch 28/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 29/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 30/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 31/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0963 - 1s/epoch - 18ms/step\n",
            "Epoch 32/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 33/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0963 - 2s/epoch - 18ms/step\n",
            "Epoch 34/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0962 - 2s/epoch - 18ms/step\n",
            "Epoch 35/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 36/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 37/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0962 - 2s/epoch - 19ms/step\n",
            "Epoch 38/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 39/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 40/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 18ms/step\n",
            "Epoch 41/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 42/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0962 - 2s/epoch - 19ms/step\n",
            "Epoch 43/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 44/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 45/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 46/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 19ms/step\n",
            "Epoch 47/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 48/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 49/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0963 - 2s/epoch - 19ms/step\n",
            "Epoch 50/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 51/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 52/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 53/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 19ms/step\n",
            "Epoch 54/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 20ms/step\n",
            "Epoch 55/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0964 - 2s/epoch - 19ms/step\n",
            "Epoch 56/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 18ms/step\n",
            "Epoch 57/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0956 - 1s/epoch - 18ms/step\n",
            "Epoch 58/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 59/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 60/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 19ms/step\n",
            "Epoch 61/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 62/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 20ms/step\n",
            "Epoch 63/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0957 - 2s/epoch - 19ms/step\n",
            "Epoch 64/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0957 - 2s/epoch - 18ms/step\n",
            "Epoch 65/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 66/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 18ms/step\n",
            "Epoch 67/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 68/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 18ms/step\n",
            "Epoch 69/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 18ms/step\n",
            "Epoch 70/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 71/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 72/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 73/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 74/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 75/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0962 - 1s/epoch - 18ms/step\n",
            "Epoch 76/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0964 - 1s/epoch - 18ms/step\n",
            "Epoch 77/100\n",
            "82/82 - 2s - loss: 0.9761 - val_loss: 1.0962 - 2s/epoch - 18ms/step\n",
            "Epoch 78/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0963 - 2s/epoch - 19ms/step\n",
            "Epoch 79/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 19ms/step\n",
            "Epoch 80/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 81/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0958 - 2s/epoch - 18ms/step\n",
            "Epoch 82/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 83/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 84/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 85/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 86/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 18ms/step\n",
            "Epoch 87/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 88/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0963 - 2s/epoch - 18ms/step\n",
            "Epoch 89/100\n",
            "82/82 - 1s - loss: 0.9761 - val_loss: 1.0960 - 1s/epoch - 18ms/step\n",
            "Epoch 90/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0962 - 2s/epoch - 18ms/step\n",
            "Epoch 91/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0961 - 1s/epoch - 18ms/step\n",
            "Epoch 92/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0959 - 2s/epoch - 18ms/step\n",
            "Epoch 93/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0962 - 2s/epoch - 18ms/step\n",
            "Epoch 94/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 19ms/step\n",
            "Epoch 95/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0960 - 2s/epoch - 19ms/step\n",
            "Epoch 96/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0962 - 2s/epoch - 19ms/step\n",
            "Epoch 97/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0958 - 1s/epoch - 18ms/step\n",
            "Epoch 98/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0957 - 1s/epoch - 18ms/step\n",
            "Epoch 99/100\n",
            "82/82 - 1s - loss: 0.9760 - val_loss: 1.0959 - 1s/epoch - 18ms/step\n",
            "Epoch 100/100\n",
            "82/82 - 2s - loss: 0.9760 - val_loss: 1.0961 - 2s/epoch - 18ms/step\n",
            "163/163 [==============================] - 1s 3ms/step\n",
            "Average MSE for the rmsprop optimizer and swish activation function: 1.0960785395736907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mse_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKSpLkyEJ4QK",
        "outputId": "0b6f27d4-8be0-493e-9b83-b3a52c535d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.6985966147239215, 1.2893571279908824, 1.8843867844482098, 1.754063754330002, 1.584303399304113, 1.0959533066900087, 1.7860286924905349, 1.567491279898536, 1.096217260216236, 1.6995887848253155, 1.667131366955679, 1.6487379253169203, 1.0960054157654937, 1.0960785395736907]\n"
          ]
        }
      ]
    }
  ]
}